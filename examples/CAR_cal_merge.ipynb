{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758d3890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged file saved to: /Users/chunghyunhan/Projects/agentics/Decision_runs_result_saving/decision_runs_consolidated_merged_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === File paths ===\n",
    "file1 = \"/Users/chunghyunhan/Projects/agentics/Decision_runs_result_saving/decision_runs_consolidated.csv\"\n",
    "file2 = \"/Users/chunghyunhan/Projects/agentics/dao_finished_proposals_stats.csv\"\n",
    "outfile = \"/Users/chunghyunhan/Projects/agentics/Decision_runs_result_saving/decision_runs_consolidated_merged_test.csv\"\n",
    "\n",
    "# === Load CSVs ===\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "# === Extract proposal_id from snapshot_url in file1 ===\n",
    "df1[\"proposal_id\"] = df1[\"snapshot_url\"].str.extract(r\"proposal/([a-fA-F0-9x]+)\")\n",
    "\n",
    "# === Merge: bring space, proposal_id, and end_iso from file2 ===\n",
    "df_merged = df1.merge(\n",
    "    df2[[\"proposal_id\", \"space\", \"end_iso\"]], \n",
    "    on=\"proposal_id\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# === Save merged file ===\n",
    "df_merged.to_csv(outfile, index=False)\n",
    "\n",
    "print(f\"✅ Merged file saved to: {outfile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09aa5d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Event study: 100%|██████████| 319/319 [00:28<00:00, 11.16event/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events input rows: 319\n",
      "Events output rows: 319\n",
      "[OK] Saved event-study results to: /Users/chunghyunhan/Projects/agentics/Decision_runs_result_saving/decision_runs_consolidated_merged_test_eventstudy.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================== Event Study (±3d, ±7d) ==============================\n",
    "# Improvements:\n",
    "# - Handle abnormal data: if price/index <= 0 → NaN → forward fill\n",
    "# - Price impact: compare average prices in [-k,-1] vs [+1,+k] (excludes day 0)\n",
    "# - CAR (market-adjusted): sum of (token_ret - mkt_ret) over [-k,+k] (includes day 0)\n",
    "# - tqdm progress bar for event loop\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===================== Paths =====================\n",
    "FILE_EVENTS = Path(\"/Users/chunghyunhan/Projects/agentics/Decision_runs_result_saving/decision_runs_consolidated_merged_test.csv\")\n",
    "FILE_REG    = Path(\"/Users/chunghyunhan/Projects/agentics/src/agentics/assets_registry/dao_registry.csv\")\n",
    "FILE_PRICES = Path(\"/Users/chunghyunhan/Projects/agentics/cmc_historical_daily_2013_2025.parquet\")\n",
    "FILE_INDEX  = Path(\"/Users/chunghyunhan/Projects/agentics/SNP_cryptoIndex.xlsx\")\n",
    "OUT_CSV     = Path(\"/Users/chunghyunhan/Projects/agentics/Decision_runs_result_saving/decision_runs_consolidated_merged_test_eventstudy.csv\")\n",
    "\n",
    "# ===================== Load inputs =====================\n",
    "events = pd.read_csv(FILE_EVENTS)\n",
    "reg    = pd.read_csv(FILE_REG)\n",
    "prices = pd.read_parquet(FILE_PRICES)\n",
    "index_df = pd.read_excel(FILE_INDEX, sheet_name=0)\n",
    "\n",
    "# ---- Assertions ----\n",
    "assert 'space' in events.columns and 'end_iso' in events.columns\n",
    "assert 'space' in reg.columns and 'CMC_ucid' in reg.columns\n",
    "\n",
    "# Prices schema\n",
    "col_ucid  = 'ucid'\n",
    "col_date  = 'date'\n",
    "col_close = 'price_USD'   # change if needed\n",
    "for c in (col_ucid, col_date, col_close):\n",
    "    assert c in prices.columns, f\"prices missing '{c}'\"\n",
    "\n",
    "# Index schema\n",
    "index_df.columns = [str(c).strip().lower() for c in index_df.columns]\n",
    "assert 'date' in index_df.columns and 'index' in index_df.columns\n",
    "\n",
    "# ===================== Preprocess events =====================\n",
    "events['event_dt'] = pd.to_datetime(events['end_iso'], utc=True, errors='coerce').dt.date\n",
    "events['event_dt'] = pd.to_datetime(events['event_dt'])\n",
    "events['event_date_str'] = events['event_dt'].dt.strftime('%Y-%m-%d')\n",
    "events = events.merge(reg[['space','CMC_ucid']], on='space', how='left')\n",
    "\n",
    "# ===================== Preprocess prices =====================\n",
    "prices = prices.copy()\n",
    "prices[col_date] = pd.to_datetime(prices[col_date], utc=False).dt.normalize()\n",
    "prices[col_close] = pd.to_numeric(prices[col_close], errors='coerce')\n",
    "\n",
    "# abnormal 값 (<=0) → NaN → forward fill\n",
    "prices.loc[prices[col_close] <= 0, col_close] = np.nan\n",
    "prices[col_close] = prices.groupby(col_ucid)[col_close].ffill()\n",
    "\n",
    "# log-return\n",
    "prices['log_price'] = np.log(prices[col_close])\n",
    "prices['token_ret'] = prices.groupby(col_ucid)['log_price'].diff()\n",
    "prices.drop(columns=['log_price'], inplace=True)\n",
    "\n",
    "# ===================== Preprocess index =====================\n",
    "index_df = index_df.rename(columns={'date': 'mkt_date', 'index': 'mkt_index'})\n",
    "index_df['mkt_date']  = pd.to_datetime(index_df['mkt_date']).dt.normalize()\n",
    "index_df['mkt_index'] = pd.to_numeric(index_df['mkt_index'], errors='coerce')\n",
    "\n",
    "# abnormal 값 (<=0) → NaN → forward fill\n",
    "index_df.loc[index_df['mkt_index'] <= 0, 'mkt_index'] = np.nan\n",
    "index_df['mkt_index'] = index_df['mkt_index'].ffill()\n",
    "\n",
    "# log-return\n",
    "index_df = index_df.sort_values('mkt_date').reset_index(drop=True)\n",
    "index_df['log_mkt'] = np.log(index_df['mkt_index'])\n",
    "index_df['mkt_ret'] = index_df['log_mkt'].diff()\n",
    "index_df.drop(columns=['log_mkt'], inplace=True)\n",
    "\n",
    "# ===================== Helpers =====================\n",
    "def token_panel(ucid: int) -> pd.DataFrame:\n",
    "    df = prices.loc[prices[col_ucid] == ucid, [col_date, col_close, 'token_ret']].copy()\n",
    "    df = df.rename(columns={col_date: 'date', col_close: 'price'})\n",
    "    df = df.merge(index_df[['mkt_date','mkt_ret']], left_on='date', right_on='mkt_date', how='left')\n",
    "    df.drop(columns=['mkt_date'], inplace=True)\n",
    "    return df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "def pre_post_avg_price_impact(panel: pd.DataFrame, event_date: pd.Timestamp, k: int):\n",
    "    panel = panel.set_index('date')\n",
    "    pre  = panel.loc[(panel.index >= event_date - pd.Timedelta(days=k)) & (panel.index <= event_date - pd.Timedelta(days=1))]\n",
    "    post = panel.loc[(panel.index >= event_date + pd.Timedelta(days=1)) & (panel.index <= event_date + pd.Timedelta(days=k))]\n",
    "    pre_avg  = pre['price'].mean()  if not pre.empty  else np.nan\n",
    "    post_avg = post['price'].mean() if not post.empty else np.nan\n",
    "    impact_pct = (post_avg - pre_avg) / pre_avg if pd.notna(pre_avg) and pre_avg != 0 else np.nan\n",
    "    return pre_avg, post_avg, impact_pct\n",
    "\n",
    "def window_sum(series: pd.Series, start_date: pd.Timestamp, a: int, b: int):\n",
    "    idx = series.index\n",
    "    left  = start_date + pd.Timedelta(days=a)\n",
    "    right = start_date + pd.Timedelta(days=b)\n",
    "    return series.loc[(idx >= left) & (idx <= right)].sum()\n",
    "\n",
    "def event_abnormal_returns(panel: pd.DataFrame, event_date: pd.Timestamp, a: int, b: int):\n",
    "    panel = panel.set_index('date')\n",
    "    if 'token_ret' not in panel or 'mkt_ret' not in panel:\n",
    "        return np.nan, np.nan\n",
    "    ar  = panel['token_ret'] - panel['mkt_ret']\n",
    "    car = window_sum(ar, event_date, a, b)              # includes day 0\n",
    "    crr = window_sum(panel['token_ret'], event_date, a, b)\n",
    "    return car, crr\n",
    "\n",
    "# ===================== Compute per-event =====================\n",
    "out_rows = []\n",
    "for _, row in tqdm(events.iterrows(), total=len(events), desc=\"Event study\", unit=\"event\"):\n",
    "    ucid = row.get('CMC_ucid', np.nan)\n",
    "    event_dt = row.get('event_dt', pd.NaT)\n",
    "\n",
    "    na_result = {\n",
    "        **row.to_dict(),\n",
    "        'pre_avg_price_3': np.nan, 'post_avg_price_3': np.nan, 'price_impact_pct_3d': np.nan,\n",
    "        'pre_avg_price_7': np.nan, 'post_avg_price_7': np.nan, 'price_impact_pct_7d': np.nan,\n",
    "        'CAR_madj_[-3,+3]': np.nan, 'CRR_raw_[-3,+3]': np.nan,\n",
    "        'CAR_madj_[-7,+7]': np.nan, 'CRR_raw_[-7,+7]': np.nan,\n",
    "    }\n",
    "    if pd.isna(ucid) or pd.isna(event_dt):\n",
    "        out_rows.append(na_result); continue\n",
    "\n",
    "    try:\n",
    "        ucid_int = int(str(ucid).strip())\n",
    "    except Exception:\n",
    "        out_rows.append(na_result); continue\n",
    "\n",
    "    panel = token_panel(ucid_int)\n",
    "    if panel.empty:\n",
    "        out_rows.append(na_result); continue\n",
    "\n",
    "    pre3, post3, imp3 = pre_post_avg_price_impact(panel, pd.to_datetime(event_dt), 3)\n",
    "    pre7, post7, imp7 = pre_post_avg_price_impact(panel, pd.to_datetime(event_dt), 7)\n",
    "    car3, crr3 = event_abnormal_returns(panel, pd.to_datetime(event_dt), -3, +3)\n",
    "    car7, crr7 = event_abnormal_returns(panel, pd.to_datetime(event_dt), -7, +7)\n",
    "    \n",
    "    car3_pct = np.expm1(car3) * 100 if pd.notna(car3) else np.nan\n",
    "    car7_pct = np.expm1(car7) * 100 if pd.notna(car7) else np.nan\n",
    "    crr3_pct = np.expm1(crr3) * 100 if pd.notna(crr3) else np.nan\n",
    "    crr7_pct = np.expm1(crr7) * 100 if pd.notna(crr7) else np.nan\n",
    "\n",
    "    out_rows.append({\n",
    "    **row.to_dict(),\n",
    "    'pre_avg_price_3': pre3, 'post_avg_price_3': post3, 'price_impact_pct_3d': imp3,\n",
    "    'pre_avg_price_7': pre7, 'post_avg_price_7': post7, 'price_impact_pct_7d': imp7,\n",
    "    'CAR_madj_[-3,+3]': car3, 'CAR_madj_[-3,+3]_pct': car3_pct,\n",
    "    'CRR_raw_[-3,+3]': crr3, 'CRR_raw_[-3,+3]_pct': crr3_pct,\n",
    "    'CAR_madj_[-7,+7]': car7, 'CAR_madj_[-7,+7]_pct': car7_pct,\n",
    "    'CRR_raw_[-7,+7]': crr7, 'CRR_raw_[-7,+7]_pct': crr7_pct,\n",
    "})\n",
    "\n",
    "out = pd.DataFrame(out_rows)\n",
    "\n",
    "# ===================== Save =====================\n",
    "print(\"Events input rows:\", len(events))\n",
    "print(\"Events output rows:\", len(out))\n",
    "assert len(out) == len(events), \"Row count mismatch between input events and output results.\"\n",
    "\n",
    "out.to_csv(OUT_CSV, index=False)\n",
    "print(f\"[OK] Saved event-study results to: {OUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
