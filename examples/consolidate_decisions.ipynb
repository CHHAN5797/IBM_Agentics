{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4e69e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_DIR: /Users/chunghyunhan/Projects/agentics/Decision_runs_result_saving\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# === Configuration ===\n",
    "ROOT_DIR = \"/Users/chunghyunhan/Projects/agentics/Decision_runs_result_saving\"  # <<-- change to your folder\n",
    "OUT_CSV  = os.path.join(ROOT_DIR, \"decision_runs_consolidated.csv\")\n",
    "OUT_JSONL= os.path.join(ROOT_DIR, \"decision_runs_consolidated.jsonl\")\n",
    "\n",
    "# If True, reprocess existing snapshot_urls so schema changes (new columns) are reflected.\n",
    "UPSERT_ON_SCHEMA_CHANGE = True\n",
    "print(\"ROOT_DIR:\", ROOT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2a5b2161",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 18) (2342366468.py, line 18)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[112]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn \"\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 18)\n"
     ]
    }
   ],
   "source": [
    "# === Helpers ===\n",
    "import json, os, glob, pandas as pd\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "def safe_get(d: Dict[str, Any], path: List[Any], default=None):\n",
    "    cur = d\n",
    "    for p in path:\n",
    "        if isinstance(cur, dict) and p in cur:\n",
    "            cur = cur[p]\n",
    "        else:\n",
    "            return default\n",
    "    return cur\n",
    "\n",
    "def flatten_evidence_quotes(evidence_list):\n",
    "    if not isinstance(evidence_list, list):\n",
    "        return \"\"\n",
    "    return \"\n",
    "\".join([f\"- {ev.get(\\\"quote\\\").strip()}\" for ev in evidence_list if isinstance(ev, dict) and ev.get(\"quote\")])\n",
    "\n",
    "def collect_similar_ids(sim_list):\n",
    "    if not isinstance(sim_list, list):\n",
    "        return \"\"\n",
    "    ids = []\n",
    "    for x in sim_list:\n",
    "        pid = (x.get(\"proposal_id\") or (x.get(\"cleaned\") or {}).get(\"proposal_id\"))\n",
    "        if pid:\n",
    "            ids.append(pid)\n",
    "    return \",\".join(ids)\n",
    "\n",
    "def build_similar_cleaned_columns(sim_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Flatten ALL 'cleaned' fields from similar_proposals_data with numbered prefixes,\n",
    "    EXCEPT 'winning_option_index' which we intentionally drop.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    if not isinstance(sim_list, list):\n",
    "        return out\n",
    "    for i, item in enumerate(sim_list, start=1):\n",
    "        cleaned = item.get(\"cleaned\") or {}\n",
    "        for k, v in cleaned.items():\n",
    "            if k == \"winning_option_index\":  # <- DROP this key\n",
    "                continue\n",
    "            col = f\"similar_proposal_{i}_{k}\"\n",
    "            out[col] = v\n",
    "    return out\n",
    "\n",
    "def build_similar_raw_impacts(sim_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create per-index raw impact columns instead of list columns:\n",
    "      similar_proposal_{i}_price_impact_pct_raw\n",
    "      similar_proposal_{i}_tvl_impact_pct_raw\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    if not isinstance(sim_list, list):\n",
    "        return out\n",
    "    for i, item in enumerate(sim_list, start=1):\n",
    "        raw = item.get(\"raw\") or {}\n",
    "        tvl_pct   = (raw.get(\"tvl_impact\") or {}).get(\"abnormal_change_pct\")\n",
    "        price_pct = (raw.get(\"price_impact\") or {}).get(\"abnormal_change_pct\")\n",
    "        if price_pct is not None:\n",
    "            out[f\"similar_proposal_{i}_price_impact_pct_raw\"] = price_pct\n",
    "        if tvl_pct is not None:\n",
    "            out[f\"similar_proposal_{i}_tvl_impact_pct_raw\"]   = tvl_pct\n",
    "    return out\n",
    "\n",
    "def build_similar_market_columns(sim_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    out = {}\n",
    "    if not isinstance(sim_list, list):\n",
    "        return out\n",
    "    for i, item in enumerate(sim_list, start=1):\n",
    "        for key in (\"price_impact_pct\", \"price_impact_market_pct\", \"price_impact_market_adjusted_pct\", \"tvl_impact_pct\"):\n",
    "            value = item.get(key)\n",
    "            if value is not None:\n",
    "                out[f\"similar_proposal_{i}_{key}\"] = value\n",
    "    return out\n",
    "\n",
    "def parse_snapshot_url(snapshot_url: str) -> Dict[str, Optional[str]]:\n",
    "    if not isinstance(snapshot_url, str):\n",
    "        return {\"proposal_id\": None, \"space\": None}\n",
    "    match = re.search(r\"#/([^/]+)/proposal/([^/?#]+)\", snapshot_url)\n",
    "    if match:\n",
    "        space, proposal_id = match.group(1), match.group(2)\n",
    "        return {\"proposal_id\": proposal_id, \"space\": space}\n",
    "    return {\"proposal_id\": None, \"space\": None}\n",
    "\n",
    "def extract_record(d: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    snapshot_url = d.get(\"snapshot_url\") or safe_get(d, [\"decision\",\"snapshot_url\"], \"\")\n",
    "    tmc = d.get(\"timeline_metrics_current\") or {}\n",
    "    decision = d.get(\"decision\", {})\n",
    "    simdata  = d.get(\"similar_proposals_data\", [])\n",
    "\n",
    "    avr = decision.get(\"actual_vote_result\") or {}\n",
    "    sim_props = decision.get(\"similar_proposals\") or []\n",
    "\n",
    "    snapshot_parts = parse_snapshot_url(snapshot_url)\n",
    "    row = {\n",
    "        \"snapshot_url\": snapshot_url,\n",
    "        \"proposal_id\": snapshot_parts[\"proposal_id\"],\n",
    "        \"space\": snapshot_parts[\"space\"],\n",
    "        \"end_iso\": decision.get(\"event_end_utc\"),\n",
    "        \"total_votes\": tmc.get(\"total_votes\"),\n",
    "        \"vp_by_quartile\": json.dumps(tmc.get(\"vp_by_quartile\")) if tmc.get(\"vp_by_quartile\") is not None else None,\n",
    "        \"spike_index\": tmc.get(\"spike_index\"),\n",
    "        \"spike_follow_support_ratio\": tmc.get(\"spike_follow_support_ratio\"),\n",
    "        \"stairwise_ratio\": tmc.get(\"stairwise_ratio\"),\n",
    "        \"half_slope_diff\": tmc.get(\"half_slope_diff\"),\n",
    "\n",
    "        # ⛔️ (변경) 더 이상 list 컬럼을 만들지 않음\n",
    "        # \"similar_raw_tvl_impact_pct_list\": ...\n",
    "        # \"similar_raw_price_impact_pct_list\": ...\n",
    "\n",
    "        \"selected_choice_label\": decision.get(\"selected_choice_label\"),\n",
    "        \"summary\": decision.get(\"summary\"),\n",
    "        \"key_arguments_for\": \" | \".join(decision.get(\"key_arguments_for\") or []),\n",
    "        \"key_arguments_against\": \" | \".join(decision.get(\"key_arguments_against\") or []),\n",
    "        \"evidence_quotes\": flatten_evidence_quotes(decision.get(\"evidence\")),\n",
    "        \"available_choices\": \", \".join(decision.get(\"available_choices\") or []) if isinstance(decision.get(\"available_choices\"), list) else decision.get(\"available_choices\"),\n",
    "\n",
    "        \"token_price_impact_pct\": decision.get(\"token_price_impact_pct\"),\n",
    "        \"token_price_market_pct\": decision.get(\"token_price_market_pct\"),\n",
    "        \"token_price_market_adjusted_pct\": decision.get(\"token_price_market_adjusted_pct\"),\n",
    "        \"tvl_impact_pct\": decision.get(\"tvl_impact_pct\"),\n",
    "        \"actual_vote_result_winner\": avr.get(\"winner_label\"),\n",
    "        \"actual_vote_scores_total\": avr.get(\"scores_total\"),\n",
    "        \"actual_vote_margin_pct\": avr.get(\"margin_pct\"),\n",
    "\n",
    "        \"decision_stance\": decision.get(\"decision_stance\"),\n",
    "        \"ai_final_conclusion\": decision.get(\"ai_final_conclusion\"),\n",
    "        \"ai_final_reason\": decision.get(\"ai_final_reason\"),\n",
    "        \"similar_proposals\": collect_similar_ids(sim_props),\n",
    "        \"ex_post_price_impact_pct\": decision.get(\"ex_post_price_impact_pct\"),\n",
    "        \"ex_post_tvl_impact_pct\": decision.get(\"ex_post_tvl_impact_pct\"),\n",
    "\n",
    "        \"market_index_impact_pct\": d.get(\"market_index_impact_pct\"),\n",
    "        \"market_adjusted_price_impact_pct\": d.get(\"market_adjusted_price_impact_pct\"),\n",
    "        \"agentic_ai_choice\": d.get(\"agentic_ai_choice\"),\n",
    "        \"actual_outcome\": d.get(\"actual_outcome\"),\n",
    "        \"match_result\": d.get(\"match_result\"),\n",
    "        \"forum_sentiment_summary\": json.dumps(d.get(\"forum_sentiment_summary\")) if d.get(\"forum_sentiment_summary\") else None,\n",
    "    }\n",
    "\n",
    "    # (추가) cleaned 필드(일부 키 제외)와 raw per-index 임팩트 병합\n",
    "    row.update(build_similar_cleaned_columns(simdata))\n",
    "    row.update(build_similar_raw_impacts(simdata))\n",
    "    row.update(build_similar_market_columns(simdata))\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ae276d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NEW ADDED THIS RUN]: 2\n",
      "[TOTAL ROWS NOW]    : 2\n",
      "CSV   -> /Users/chunghyunhan/Projects/agentics/Decision_runs_result_saving/decision_runs_consolidated.csv\n",
      "JSONL -> /Users/chunghyunhan/Projects/agentics/Decision_runs_result_saving/decision_runs_consolidated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vz/g2dffj7n39s65g4gwshdmsbc0000gn/T/ipykernel_55240/2292696155.py:31: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshot_url</th>\n",
       "      <th>similar_proposal_1_author</th>\n",
       "      <th>similar_proposal_1_change_stance</th>\n",
       "      <th>similar_proposal_1_end_utc</th>\n",
       "      <th>similar_proposal_1_margin_abs</th>\n",
       "      <th>similar_proposal_1_margin_pct</th>\n",
       "      <th>similar_proposal_1_price_impact_market_adjusted_pct</th>\n",
       "      <th>similar_proposal_1_price_impact_market_pct</th>\n",
       "      <th>similar_proposal_1_price_impact_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://snapshot.org/#/aavedao.eth/proposal/0x...</td>\n",
       "      <td>0xF1dF824419879Bb8a7E758173523F88EfB7Af193</td>\n",
       "      <td>To change</td>\n",
       "      <td>2024-10-18T10:01:59Z</td>\n",
       "      <td>681545.386417</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>-0.4164</td>\n",
       "      <td>1.7272</td>\n",
       "      <td>1.3108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://snapshot.org/#/aavedao.eth/proposal/0x...</td>\n",
       "      <td>0xF1dF824419879Bb8a7E758173523F88EfB7Af193</td>\n",
       "      <td>To change</td>\n",
       "      <td>2025-07-26T11:31:29Z</td>\n",
       "      <td>777987.210231</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.3538</td>\n",
       "      <td>0.4195</td>\n",
       "      <td>0.7733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        snapshot_url  \\\n",
       "0  https://snapshot.org/#/aavedao.eth/proposal/0x...   \n",
       "1  https://snapshot.org/#/aavedao.eth/proposal/0x...   \n",
       "\n",
       "                    similar_proposal_1_author  \\\n",
       "0  0xF1dF824419879Bb8a7E758173523F88EfB7Af193   \n",
       "1  0xF1dF824419879Bb8a7E758173523F88EfB7Af193   \n",
       "\n",
       "  similar_proposal_1_change_stance similar_proposal_1_end_utc  \\\n",
       "0                        To change       2024-10-18T10:01:59Z   \n",
       "1                        To change       2025-07-26T11:31:29Z   \n",
       "\n",
       "   similar_proposal_1_margin_abs  similar_proposal_1_margin_pct  \\\n",
       "0                  681545.386417                       0.884058   \n",
       "1                  777987.210231                       0.999992   \n",
       "\n",
       "   similar_proposal_1_price_impact_market_adjusted_pct  \\\n",
       "0                                            -0.4164     \n",
       "1                                             0.3538     \n",
       "\n",
       "   similar_proposal_1_price_impact_market_pct  \\\n",
       "0                                      1.7272   \n",
       "1                                      0.4195   \n",
       "\n",
       "   similar_proposal_1_price_impact_pct  \n",
       "0                               1.3108  \n",
       "1                               0.7733  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# === Consolidate and print 'newly added' count; always overwrite outputs ===\n",
    "# 1) Load existing consolidated to know which snapshot_urls we already have\n",
    "if os.path.exists(OUT_CSV):\n",
    "    existing_df = pd.read_csv(OUT_CSV)\n",
    "    known_urls = set(existing_df.get(\"snapshot_url\", pd.Series(dtype=str)).dropna().astype(str))\n",
    "else:\n",
    "    existing_df = pd.DataFrame()\n",
    "    known_urls = set()\n",
    "\n",
    "# 2) Read ALL json files currently in the folder\n",
    "rows = []\n",
    "for fp in sorted(glob.glob(os.path.join(ROOT_DIR, \"*.json\"))):\n",
    "    try:\n",
    "        with open(fp, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception:\n",
    "        continue\n",
    "    rows.append(extract_record(data))\n",
    "\n",
    "new_df = pd.DataFrame(rows)\n",
    "\n",
    "# 3) Count how many are NEW vs existing, by snapshot_url\n",
    "file_urls = set(new_df.get(\"snapshot_url\", pd.Series(dtype=str)).dropna().astype(str))\n",
    "new_urls = file_urls - known_urls\n",
    "num_new = len(new_urls)\n",
    "\n",
    "# 4) Upsert (union schema, prefer latest rows) and ALWAYS overwrite outputs\n",
    "union_cols = sorted(set(existing_df.columns) | set(new_df.columns))\n",
    "existing_df = existing_df.reindex(columns=union_cols, fill_value=pd.NA)\n",
    "new_df      = new_df.reindex(columns=union_cols,      fill_value=pd.NA)\n",
    "final_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "if \"snapshot_url\" in final_df.columns:\n",
    "    final_df = final_df.drop_duplicates(subset=[\"snapshot_url\"], keep=\"last\")\n",
    "\n",
    "# 5) Save (overwrite)\n",
    "final_df.to_csv(OUT_CSV, index=False)\n",
    "with open(OUT_JSONL, \"w\") as f:\n",
    "    for _, r in final_df.iterrows():\n",
    "        f.write(json.dumps({k:v for k,v in r.items() if pd.notna(v)}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"[NEW ADDED THIS RUN]: {num_new}\")\n",
    "print(f\"[TOTAL ROWS NOW]    : {len(final_df)}\")\n",
    "print(f\"CSV   -> {OUT_CSV}\")\n",
    "print(f\"JSONL -> {OUT_JSONL}\")\n",
    "\n",
    "# 6) Optional: small preview\n",
    "display_cols = [c for c in final_df.columns if c.startswith(\"similar_proposal_\")][:8]\n",
    "preview = final_df[[\"snapshot_url\"] + display_cols].head(5)\n",
    "try:\n",
    "    from caas_jupyter_tools import display_dataframe_to_user\n",
    "    display_dataframe_to_user(\"Preview (first 5 rows with similar_proposal_* cols)\", preview)\n",
    "except Exception:\n",
    "    display(preview)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
