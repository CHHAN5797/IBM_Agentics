{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4fb3390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "REPO = Path(\"/Users/chunghyunhan/Projects/agentics\").resolve()\n",
    "os.chdir(REPO)\n",
    "SRC = REPO / \"src\"\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "load_dotenv(REPO / \".env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607f51c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: /Users/chunghyunhan/Projects/.venv/bin/python\n",
      "CWD   : /Users/chunghyunhan/Projects/agentics\n",
      "{'AGENTICS_LLM_PROVIDER': 'SET', 'OPENAI_API_KEY': 'SET'}\n",
      "AGENTICS_LLM_PROVIDER = openai\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "REPO = Path(\"/Users/chunghyunhan/Projects/agentics\").resolve()\n",
    "os.chdir(REPO)\n",
    "\n",
    "from pprint import pprint\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"CWD   :\", os.getcwd())\n",
    "\n",
    "# .env 강제 로드 (루트 고정)\n",
    "load_dotenv(REPO / \".env\", override=True)\n",
    "\n",
    "# 가장 중요한 키들 확인\n",
    "must = [\"OPENAI_API_KEY\", \"AGENTICS_LLM_PROVIDER\"]\n",
    "pprint({k: (\"SET\" if os.getenv(k) else None) for k in must})\n",
    "\n",
    "# (권장) provider를 명시적으로 openai로 고정\n",
    "os.environ[\"AGENTICS_LLM_PROVIDER\"] = os.getenv(\"AGENTICS_LLM_PROVIDER\") or \"openai\"\n",
    "print(\"AGENTICS_LLM_PROVIDER =\", os.environ[\"AGENTICS_LLM_PROVIDER\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dc7f347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>space</th>\n",
       "      <th>proposal_id</th>\n",
       "      <th>title</th>\n",
       "      <th>end_iso</th>\n",
       "      <th>num_voters</th>\n",
       "      <th>vp_min</th>\n",
       "      <th>vp_25%</th>\n",
       "      <th>vp_median</th>\n",
       "      <th>vp_75%</th>\n",
       "      <th>vp_max</th>\n",
       "      <th>vp_mean</th>\n",
       "      <th>vp_std</th>\n",
       "      <th>sp_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3064</th>\n",
       "      <td>lido-snapshot.eth</td>\n",
       "      <td>0xe2165bbde749b0f0bb7d8c78447eb64e5ff4e700b790...</td>\n",
       "      <td>Proposal to fund the Protocol Guild Pilot via...</td>\n",
       "      <td>2022-04-16T17:00:00+00:00</td>\n",
       "      <td>61</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.559323</td>\n",
       "      <td>396.0</td>\n",
       "      <td>20010000.0</td>\n",
       "      <td>1.467490e+06</td>\n",
       "      <td>4.795811e+06</td>\n",
       "      <td>8.951692e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  space                                        proposal_id  \\\n",
       "3064  lido-snapshot.eth  0xe2165bbde749b0f0bb7d8c78447eb64e5ff4e700b790...   \n",
       "\n",
       "                                                  title  \\\n",
       "3064   Proposal to fund the Protocol Guild Pilot via...   \n",
       "\n",
       "                        end_iso  num_voters  vp_min  vp_25%  vp_median  \\\n",
       "3064  2022-04-16T17:00:00+00:00          61     0.1     0.3   2.559323   \n",
       "\n",
       "      vp_75%      vp_max       vp_mean        vp_std        sp_sum  \n",
       "3064   396.0  20010000.0  1.467490e+06  4.795811e+06  8.951692e+07  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, re, subprocess, sys, time, os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Dict, Any, Tuple, Set\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm  # pip install tqdm\n",
    "\n",
    "# ========= Config =========\n",
    "REPO_ROOT = Path(\"/Users/chunghyunhan/Projects/agentics\").resolve()\n",
    "CSV_PATH = REPO_ROOT / \"dao_finished_proposals_stats.csv\"\n",
    "SCRIPT = REPO_ROOT / \"examples\" / \"agentics_proposal_decision.py\"\n",
    "\n",
    "DEFAULT_RUN_DIR = REPO_ROOT / \"Decision_runs\"\n",
    "DEFAULT_RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR = REPO_ROOT / \"Decision_runs_result_saving\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Performance / robustness knobs\n",
    "SUBPROC_TIMEOUT_SEC = 600           # kill if a single run hangs > 10 minutes\n",
    "SLEEP_BETWEEN_RUNS_SEC = 0.5        # spacing to respect MCP/LLM rate limits\n",
    "CHECKPOINT_EVERY = 25               # write a checkpoint parquet every N runs\n",
    "RESUME_IF_EXISTS = True             # skip proposals that already have saved_json\n",
    "FAIL_STOP_AFTER = None              # e.g., set to 10 to abort after 10 failures\n",
    "\n",
    "# ========= Load =========\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df[3064:3065]\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45d05192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1 | Remaining after resume filter: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632b04c4280e4309bea4a0d687bfd81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Snapshot proposals:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3064] lido-snapshot.eth :: 0xe2165bbde749b0f0bb7d8c78447eb64e5ff4e700b7905023fdd0eec820ebe5b4 -> https://snapshot.org/#/lido-snapshot.eth/proposal/0xe2165bbde749b0f0bb7d8c78447eb64e5ff4e700b7905023fdd0eec820ebe5b4\n",
      "    Fallback picked decision_20251002T213904Z.json from Decision_runs\n",
      "Done. 1 processed | ok=1 | skip=0 | timeout=0 | fail=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic column validation (defensive)\n",
    "for col in (\"space\", \"proposal_id\"):\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Missing required column in CSV: {col!r}\")\n",
    "\n",
    "# ========= Helpers =========\n",
    "def snapshot_url(space: str, proposal_id: str) -> str:\n",
    "    return f\"https://snapshot.org/#/{space}/proposal/{proposal_id}\"\n",
    "\n",
    "_SANITIZE = re.compile(r\"[^A-Za-z0-9._-]\")\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    return _SANITIZE.sub(\"_\", str(s))\n",
    "\n",
    "def derive_filename(space: str, index: int, proposal_id: str) -> Path:\n",
    "    \"\"\"Legacy canonical name (kept for backward-compat).\"\"\"\n",
    "    safe_space = sanitize(space)\n",
    "    safe_pid = sanitize(proposal_id)\n",
    "    return OUTPUT_DIR / f\"{safe_space}_{index:04d}_{safe_pid}.json\"\n",
    "\n",
    "def has_saved(space: str, proposal_id: str) -> bool:\n",
    "    \"\"\"\n",
    "    Index-independent existence check.\n",
    "    We glob for any file like {space}_*_{proposal_id}.json\n",
    "    so reordering/slicing of CSV won't break resume.\n",
    "    \"\"\"\n",
    "    safe_space = sanitize(space)\n",
    "    safe_pid = sanitize(proposal_id)\n",
    "    pattern = f\"{safe_space}_*_{safe_pid}.json\"\n",
    "    return any(OUTPUT_DIR.glob(pattern))\n",
    "\n",
    "def list_decision_jsons() -> Dict[str, Path]:\n",
    "    \"\"\"Current decision_* artifacts under DEFAULT_RUN_DIR.\"\"\"\n",
    "    return {p.name: p for p in DEFAULT_RUN_DIR.glob(\"decision_*.json\")}\n",
    "\n",
    "def detect_new_decision(before: Dict[str, Path]) -> Optional[Path]:\n",
    "    \"\"\"Return newest decision json created after the run.\"\"\"\n",
    "    after = list_decision_jsons()\n",
    "    new_paths = [path for name, path in after.items() if name not in before]\n",
    "    if not new_paths:\n",
    "        return None\n",
    "    return max(new_paths, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "def run_decision(url: str) -> subprocess.CompletedProcess:\n",
    "    \"\"\"Run the interactive script once with canned stdin.\"\"\"\n",
    "    canned_input = f\"{url}\\n\"   # Snapshot Proposal URL>\n",
    "    canned_input += \"n\\n\"       # reuse focus areas? -> no\n",
    "    canned_input += \"\\n\"        # custom focus (blank)\n",
    "    env = os.environ.copy()\n",
    "    src_path = str(REPO_ROOT / \"src\")\n",
    "    existing = env.get(\"PYTHONPATH\")\n",
    "    env[\"PYTHONPATH\"] = src_path if not existing else f\"{src_path}:{existing}\"\n",
    "    return subprocess.run(\n",
    "        [sys.executable, str(SCRIPT)],\n",
    "        input=canned_input,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        cwd=REPO_ROOT,\n",
    "        env=env,\n",
    "        check=False,            # don't raise; we inspect returncode\n",
    "        timeout=SUBPROC_TIMEOUT_SEC,\n",
    "    )\n",
    "\n",
    "_SAVED_REGEX = re.compile(r\"Saved:\\s*(.*Decision_runs/decision_[0-9T:-]+\\.json)\")\n",
    "\n",
    "def extract_saved_path(stdout: str) -> Optional[Path]:\n",
    "    m = _SAVED_REGEX.search(stdout or \"\")\n",
    "    return Path(m.group(1)).resolve() if m else None\n",
    "\n",
    "def summarize_status(rec: Dict[str, Any]) -> str:\n",
    "    if rec.get(\"skipped\"):\n",
    "        return \"skip\"\n",
    "    rc = rec.get(\"returncode\")\n",
    "    if rc == 0 and rec.get(\"saved_json\"):\n",
    "        return \"ok\"\n",
    "    if rec.get(\"timeout\"):\n",
    "        return \"timeout\"\n",
    "    return f\"fail(rc={rc})\"\n",
    "\n",
    "# ========= Pre-compute already-done set (index-independent) =========\n",
    "if RESUME_IF_EXISTS:\n",
    "    done_pairs: Set[Tuple[str, str]] = set()\n",
    "    # Parse existing files in OUTPUT_DIR\n",
    "    for p in OUTPUT_DIR.glob(\"*.json\"):\n",
    "        name = p.name  # e.g., aavedao.eth_0001_0xabcde....json\n",
    "        try:\n",
    "            # split by last underscore to get proposal_id; the rest is space + index\n",
    "            # safer: split from right at most once\n",
    "            head, _, tail = name.rpartition(\"_\")\n",
    "            pid = tail[:-5]  # strip \".json\"\n",
    "            space = head.split(\"_\", 1)[0]  # before first underscore is space (sanitized)\n",
    "            done_pairs.add((space, pid))\n",
    "        except Exception:\n",
    "            # if parsing fails, ignore (still covered by per-iteration has_saved)\n",
    "            pass\n",
    "else:\n",
    "    done_pairs = set()\n",
    "\n",
    "# Filter DF to remaining work (so we don't even iterate finished ones)\n",
    "if RESUME_IF_EXISTS and len(done_pairs) > 0:\n",
    "    # Compare with sanitized names to match file naming\n",
    "    df[\"_space_key\"] = df[\"space\"].map(sanitize)\n",
    "    df[\"_pid_key\"] = df[\"proposal_id\"].map(sanitize)\n",
    "    mask = ~df.apply(lambda r: (r[\"_space_key\"], r[\"_pid_key\"]) in done_pairs, axis=1)\n",
    "    df_remaining = df[mask].drop(columns=[\"_space_key\", \"_pid_key\"])\n",
    "else:\n",
    "    df_remaining = df\n",
    "\n",
    "print(f\"Total rows: {len(df)} | Remaining after resume filter: {len(df_remaining)}\")\n",
    "\n",
    "# ========= Main =========\n",
    "results = []\n",
    "fail_count = 0\n",
    "start_all = time.time()\n",
    "\n",
    "progress = tqdm(\n",
    "    df_remaining.itertuples(index=True, name=\"Proposal\"),\n",
    "    total=len(df_remaining),\n",
    "    desc=\"Processing Snapshot proposals\",\n",
    "    dynamic_ncols=True,\n",
    "    leave=True,\n",
    ")\n",
    "\n",
    "for row in progress:\n",
    "    idx = int(row.Index)\n",
    "    space = str(row.space)\n",
    "    pid = str(row.proposal_id)\n",
    "    url = snapshot_url(space, pid)\n",
    "\n",
    "    # Index-independent skip (fast path)\n",
    "    if RESUME_IF_EXISTS and has_saved(space, pid):\n",
    "        rec = {\n",
    "            \"index\": idx,\n",
    "            \"space\": space,\n",
    "            \"proposal_id\": pid,\n",
    "            \"snapshot_url\": url,\n",
    "            \"returncode\": 0,\n",
    "            \"stdout\": \"\",\n",
    "            \"stderr\": \"\",\n",
    "            \"saved_json\": str(next(OUTPUT_DIR.glob(f\"{sanitize(space)}_*_{sanitize(pid)}.json\"))),\n",
    "            \"skipped\": True,\n",
    "            \"timeout\": False,\n",
    "            \"started_at\": None,\n",
    "            \"ended_at\": None,\n",
    "            \"elapsed_sec\": 0.0,\n",
    "        }\n",
    "        results.append(rec)\n",
    "        progress.set_postfix({\"status\": \"skip\", \"space\": space, \"id\": pid[:8]})\n",
    "        continue\n",
    "\n",
    "    # For new runs, also compute canonical target (kept for continuity)\n",
    "    target_path = derive_filename(space, idx, pid)\n",
    "\n",
    "    # Show what's being processed right now\n",
    "    progress.set_postfix({\"status\": \"run\", \"space\": space, \"id\": pid[:8]})\n",
    "    tqdm.write(f\"[{idx}] {space} :: {pid} -> {url}\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    started_at = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    proc = None\n",
    "    timeout_hit = False\n",
    "    before_decisions = list_decision_jsons()\n",
    "    try:\n",
    "        proc = run_decision(url)\n",
    "    except subprocess.TimeoutExpired as te:\n",
    "        timeout_hit = True\n",
    "        proc = te\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    ended_at = datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    # Try to pull the saved path from stdout\n",
    "    saved_path = None\n",
    "    stdout_text = getattr(proc, \"stdout\", \"\")\n",
    "    stderr_text = getattr(proc, \"stderr\", \"\")\n",
    "    saved_path = extract_saved_path(stdout_text or \"\")\n",
    "    saved_origin = \"stdout\" if saved_path else None\n",
    "\n",
    "    if (not saved_path or not saved_path.exists()) and not timeout_hit:\n",
    "        fallback_path = detect_new_decision(before_decisions)\n",
    "        if fallback_path and fallback_path.exists():\n",
    "            saved_path = fallback_path\n",
    "            saved_origin = \"fallback\"\n",
    "            tqdm.write(f\"    Fallback picked {fallback_path.name} from Decision_runs\")\n",
    "\n",
    "    renamed_path = None\n",
    "    if saved_path and saved_path.exists():\n",
    "        # Move into our OUTPUT_DIR with canonical name\n",
    "        target_path.write_bytes(saved_path.read_bytes())\n",
    "        try:\n",
    "            saved_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        renamed_path = target_path\n",
    "\n",
    "    rec = {\n",
    "        \"index\": idx,\n",
    "        \"space\": space,\n",
    "        \"proposal_id\": pid,\n",
    "        \"snapshot_url\": url,\n",
    "        \"returncode\": getattr(proc, \"returncode\", None),\n",
    "        \"stdout\": stdout_text,\n",
    "        \"stderr\": stderr_text,\n",
    "        \"saved_json\": str(renamed_path) if renamed_path else None,\n",
    "        \"saved_json_source\": saved_origin,\n",
    "        \"skipped\": False,\n",
    "        \"timeout\": timeout_hit,\n",
    "        \"started_at\": started_at,\n",
    "        \"ended_at\": ended_at,\n",
    "        \"elapsed_sec\": round(elapsed, 3),\n",
    "    }\n",
    "    results.append(rec)\n",
    "\n",
    "    status = summarize_status(rec)\n",
    "    progress.set_postfix({\"status\": status, \"t\": f\"{elapsed:.1f}s\", \"space\": space, \"id\": pid[:8]})\n",
    "\n",
    "    if status.startswith(\"fail\"):\n",
    "        fail_count += 1\n",
    "        tqdm.write(f\"  -> FAIL ({status}). See stderr below:\")\n",
    "        if stderr_text:\n",
    "            head = \"\\n\".join(stderr_text.splitlines()[:12])\n",
    "            tqdm.write(head)\n",
    "        fail_log = OUTPUT_DIR / f\"fail_{idx:04d}_{sanitize(space)}_{sanitize(pid)[:8]}.log\"\n",
    "        try:\n",
    "            fail_log.write_text(\n",
    "                f\"URL: {url}\\nReturnCode: {rec['returncode']}\\nTimeout: {timeout_hit}\\n\"\n",
    "                f\"--- STDOUT ---\\n{stdout_text}\\n\\n--- STDERR ---\\n{stderr_text}\\n\"\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "        if FAIL_STOP_AFTER and fail_count >= FAIL_STOP_AFTER:\n",
    "            tqdm.write(f\"Aborting after {fail_count} failures (FAIL_STOP_AFTER).\")\n",
    "            break\n",
    "\n",
    "    # checkpoint save\n",
    "    if len(results) % CHECKPOINT_EVERY == 0:\n",
    "        ckpt = pd.DataFrame(results)\n",
    "        ckpt.to_parquet(OUTPUT_DIR / \"run_log_ckpt.parquet\", index=False)\n",
    "        tqdm.write(f\"[checkpoint] wrote {len(results)} records\")\n",
    "\n",
    "    # spacing to be gentle with MCP/LLM rate limits\n",
    "    time.sleep(SLEEP_BETWEEN_RUNS_SEC)\n",
    "\n",
    "# Final save\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_parquet(OUTPUT_DIR / \"run_log.parquet\", index=False)\n",
    "\n",
    "tqdm.write(\n",
    "    f\"Done. {len(results)} processed | \"\n",
    "    f\"ok={sum(1 for r in results if summarize_status(r)=='ok')} | \"\n",
    "    f\"skip={sum(1 for r in results if r.get('skipped'))} | \"\n",
    "    f\"timeout={sum(1 for r in results if r.get('timeout'))} | \"\n",
    "    f\"fail={sum(1 for r in results if summarize_status(r).startswith('fail'))}\"\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
