{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4102f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Snapshot í”„ë¡œí¬ì ˆ ìˆ˜ì§‘ ì‹œì‘...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Spaces: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:30<00:00,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ì™„ë£Œ!\n",
      " - Target_proposals/summary_with_discussion_by_space.csv\n",
      " - Target_proposals/<space>_with_discussion.csv (per space)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "from tqdm import tqdm   # ğŸ”¹ tqdm ì¶”ê°€\n",
    "\n",
    "# ---------- ê¸°ë³¸ ì„¤ì • ----------\n",
    "SNAPSHOT_API = \"https://hub.snapshot.org/graphql\"\n",
    "TIMEOUT = 30\n",
    "BASE_SLEEP  = 0.6\n",
    "MAX_RETRIES = 5\n",
    "BACKOFF_BASE = 1.7\n",
    "JITTER = (0.1, 0.35)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"target-proposals/0.1\"})\n",
    "\n",
    "# ---------- ìœ í‹¸ ----------\n",
    "def _sleep():\n",
    "    time.sleep(BASE_SLEEP + random.uniform(*JITTER))\n",
    "\n",
    "def _now_ts() -> int:\n",
    "    return int(datetime.now(timezone.utc).timestamp())\n",
    "\n",
    "def _ts_to_iso(ts: int) -> str:\n",
    "    return datetime.fromtimestamp(int(ts), tz=timezone.utc).isoformat()\n",
    "\n",
    "# ---------- Snapshot GraphQL ----------\n",
    "PROPOSALS_Q = \"\"\"\n",
    "query($space: String!, $first: Int!, $skip: Int!) {\n",
    "  proposals(\n",
    "    first: $first\n",
    "    skip: $skip\n",
    "    where: { space_in: [$space] }\n",
    "    orderBy: \"created\"\n",
    "    orderDirection: desc\n",
    "  ) {\n",
    "    id\n",
    "    title\n",
    "    author\n",
    "    body\n",
    "    discussion\n",
    "    start\n",
    "    end\n",
    "    state\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def gql(query: str, variables: Optional[dict] = None) -> dict:\n",
    "    retries = 0\n",
    "    while True:\n",
    "        _sleep()\n",
    "        try:\n",
    "            r = session.post(SNAPSHOT_API, json={\"query\": query, \"variables\": variables or {}}, timeout=TIMEOUT)\n",
    "        except requests.RequestException:\n",
    "            if retries < MAX_RETRIES:\n",
    "                delay = (BACKOFF_BASE ** retries) + random.uniform(*JITTER)\n",
    "                time.sleep(delay)\n",
    "                retries += 1\n",
    "                continue\n",
    "            raise\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        if r.status_code in (429, 502, 503, 504) and retries < MAX_RETRIES:\n",
    "            ra = r.headers.get(\"Retry-After\")\n",
    "            delay = float(ra) if (ra and ra.isdigit()) else (BACKOFF_BASE ** retries)\n",
    "            time.sleep(delay + random.uniform(*JITTER))\n",
    "            retries += 1\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "\n",
    "def fetch_all_proposals(space: str, batch: int = 100) -> List[dict]:\n",
    "    out, skip = [], 0\n",
    "    while True:\n",
    "        data = gql(PROPOSALS_Q, {\"space\": space, \"first\": batch, \"skip\": skip})\n",
    "        if not data or \"data\" not in data:\n",
    "            break\n",
    "        chunk = data[\"data\"].get(\"proposals\", [])\n",
    "        if not chunk:\n",
    "            break\n",
    "        out.extend(chunk)\n",
    "        if len(chunk) < batch:\n",
    "            break\n",
    "        skip += batch\n",
    "    return out\n",
    "\n",
    "def finished_only(proposals: List[dict]) -> List[dict]:\n",
    "    nt = _now_ts()\n",
    "    return [p for p in proposals if p.get(\"state\") == \"closed\" and int(p.get(\"end\") or 0) <= nt]\n",
    "\n",
    "def with_discussion_only(proposals: List[dict]) -> List[dict]:\n",
    "    \"\"\"Snapshotì˜ discussion í•„ë“œê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²ƒë§Œ\"\"\"\n",
    "    return [p for p in proposals if (p.get(\"discussion\") or \"\").strip()]\n",
    "\n",
    "# ---------- ì‹¤í–‰ íŒŒíŠ¸ ----------\n",
    "SPACES = [\n",
    "    \"aavedao.eth\",\n",
    "    \"arbitrumfoundation.eth\",\n",
    "    \"snapshot.dcl.eth\",\n",
    "    \"balancer.eth\",\n",
    "    \"cvx.eth\",\n",
    "    \"1inch.eth\",\n",
    "    \"aurafinance.eth\",\n",
    "    \"lido-snapshot.eth\",\n",
    "    \"uniswapgovernance.eth\",\n",
    "    \"metislayer2.eth\",\n",
    "]\n",
    "\n",
    "os.makedirs(\"Target_proposals\", exist_ok=True)\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "print(\"ğŸš€ Snapshot í”„ë¡œí¬ì ˆ ìˆ˜ì§‘ ì‹œì‘...\\n\")\n",
    "\n",
    "# tqdmìœ¼ë¡œ í”„ë¡œì„¸ìŠ¤ ì§„í–‰ë¥  í‘œì‹œ\n",
    "for space in tqdm(SPACES, desc=\"Processing Spaces\"):\n",
    "    # 1) í•´ë‹¹ spaceì˜ ì „ì²´ í”„ë¡œí¬ì ˆ ê°€ì ¸ì˜¤ê¸°\n",
    "    all_props = fetch_all_proposals(space)\n",
    "    \n",
    "    # 2) ì¢…ë£Œ(closed) ìƒíƒœë§Œ ë‚¨ê¸°ê¸°\n",
    "    finished_props = finished_only(all_props)\n",
    "    \n",
    "    # 3) discussion í•„ë“œê°€ ìˆëŠ” í”„ë¡œí¬ì ˆë§Œ ë‚¨ê¸°ê¸°\n",
    "    discussion_props = with_discussion_only(finished_props)\n",
    "\n",
    "    # 4) spaceë³„ ê°œë³„ CSV ì €ì¥\n",
    "    df = pd.DataFrame([{\n",
    "        \"space\": space,\n",
    "        \"id\": p.get(\"id\"),\n",
    "        \"title\": p.get(\"title\"),\n",
    "        \"author\": p.get(\"author\"),\n",
    "        \"discussion\": p.get(\"discussion\"),\n",
    "        \"start\": int(p.get(\"start\") or 0),\n",
    "        \"end\": int(p.get(\"end\") or 0),\n",
    "        \"end_iso\": _ts_to_iso(p.get(\"end\") or 0),\n",
    "        \"state\": p.get(\"state\"),\n",
    "    } for p in discussion_props])\n",
    "    df.to_csv(f\"Target_proposals/{space}_with_discussion.csv\", index=False)\n",
    "\n",
    "    # 5) summary ì •ë³´ ì—…ë°ì´íŠ¸\n",
    "    total = len(finished_props)  # æ¯ìˆ˜: ì¢…ë£Œëœ í”„ë¡œí¬ì ˆ ìˆ˜\n",
    "    count_with_disc = len(discussion_props)\n",
    "    percent = (count_with_disc / total * 100.0) if total > 0 else 0.0\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"space\": space,\n",
    "        \"total_closed\": total,\n",
    "        \"with_discussion\": count_with_disc,\n",
    "        \"without_discussion\": max(total - count_with_disc, 0),\n",
    "        \"pct_with_discussion\": round(percent, 2),\n",
    "    })\n",
    "\n",
    "# ---------- ìš”ì•½ ì €ì¥ ----------\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"pct_with_discussion\", ascending=False)\n",
    "summary_df.to_csv(\"Target_proposals/summary_with_discussion_by_space.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ… ì™„ë£Œ!\")\n",
    "print(\" - Target_proposals/summary_with_discussion_by_space.csv\")\n",
    "print(\" - Target_proposals/<space>_with_discussion.csv (per space)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df8e52c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ìƒ˜í”Œ 10ê°œ ëŒ“ê¸€ ì €ì¥ ì‹œì‘...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving comments: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:25<00:00,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ì™„ë£Œ!\n",
      " - í”„ë¡œí¬ì ˆë³„ ëŒ“ê¸€ CSV: Target_proposals/comments/<space>__<proposal_id>__comments.csv\n",
      " - í†µí•© CSV: Target_proposals/comments/comments_all.csv\n",
      " - í†µí•© JSONL(LLM ì¹œí™”): Target_proposals/comments/comments_all.jsonl\n",
      " - ìŠ¤í‚µ ë¡œê·¸(ë¹„-ë””ìŠ¤ì½”ìŠ¤): Target_proposals/comments/skipped_non_discourse.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, glob, json, time, random, re, requests\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------- HTTP & ìœ í‹¸ --------\n",
    "TIMEOUT = 30\n",
    "BASE_SLEEP  = 0.6\n",
    "MAX_RETRIES = 5\n",
    "BACKOFF_BASE = 1.7\n",
    "JITTER = (0.1, 0.35)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"target-proposals-save-comments/0.1\"})\n",
    "\n",
    "def _sleep():\n",
    "    time.sleep(BASE_SLEEP + random.uniform(*JITTER))\n",
    "\n",
    "def fetch_url(url: str) -> requests.Response:\n",
    "    retries = 0\n",
    "    while True:\n",
    "        _sleep()\n",
    "        try:\n",
    "            r = session.get(url, timeout=TIMEOUT)\n",
    "        except requests.RequestException:\n",
    "            if retries < MAX_RETRIES:\n",
    "                delay = (BACKOFF_BASE ** retries) + random.uniform(*JITTER)\n",
    "                time.sleep(delay); retries += 1\n",
    "                continue\n",
    "            raise\n",
    "        if r.status_code == 200:\n",
    "            return r\n",
    "        if r.status_code in (429, 502, 503, 504) and retries < MAX_RETRIES:\n",
    "            ra = r.headers.get(\"Retry-After\")\n",
    "            delay = float(ra) if (ra and ra.isdigit()) else (BACKOFF_BASE ** retries)\n",
    "            time.sleep(delay + random.uniform(*JITTER)); retries += 1\n",
    "            continue\n",
    "        return r\n",
    "\n",
    "def is_discourse_thread(url: str) -> bool:\n",
    "    try:\n",
    "        return url.startswith(\"http\") and \"/t/\" in urlparse(url).path.lower()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def to_ts(iso: str) -> int:\n",
    "    try:\n",
    "        return int(datetime.fromisoformat(iso.replace(\"Z\",\"+00:00\")).timestamp())\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def fetch_discourse_thread(url: str, max_pages: int = 10):\n",
    "    base = url.split(\"?\")[0].rstrip(\"/\")\n",
    "    if not base.endswith(\".json\"):\n",
    "        base = base + \".json\"\n",
    "    posts, header = [], {}\n",
    "    for page in range(1, max_pages + 1):\n",
    "        u = base if page == 1 else base.replace(\".json\", f\".json?page={page}\")\n",
    "        rr = fetch_url(u)\n",
    "        if rr.status_code != 200:\n",
    "            break\n",
    "        j = rr.json()\n",
    "        if page == 1:\n",
    "            header = {\n",
    "                \"title\": j.get(\"title\"),\n",
    "                \"slug\": j.get(\"slug\"),\n",
    "                \"created_at\": j.get(\"created_at\"),\n",
    "                \"posts_count\": j.get(\"posts_count\"),\n",
    "                \"tags\": j.get(\"tags\"),\n",
    "                \"url\": url,\n",
    "            }\n",
    "        chunk = j.get(\"post_stream\", {}).get(\"posts\", [])\n",
    "        if not chunk:\n",
    "            break\n",
    "        for p in chunk:\n",
    "            posts.append({\n",
    "                \"id\": p.get(\"id\"),\n",
    "                \"username\": p.get(\"username\"),\n",
    "                \"user_id\": p.get(\"user_id\"),\n",
    "                \"created_at\": p.get(\"created_at\"),\n",
    "                \"updated_at\": p.get(\"updated_at\"),\n",
    "                \"raw\": p.get(\"raw\"),\n",
    "                \"cooked\": p.get(\"cooked\"),\n",
    "                \"post_number\": p.get(\"post_number\"),\n",
    "                \"reply_to_post_number\": p.get(\"reply_to_post_number\"),\n",
    "            })\n",
    "    return {\"thread\": header, \"posts\": posts}\n",
    "\n",
    "# -------- ì…ë ¥ ë¡œë“œ & ìƒ˜í”Œë§ --------\n",
    "os.makedirs(\"Target_proposals\", exist_ok=True)\n",
    "comments_dir = \"Target_proposals/comments\"\n",
    "os.makedirs(comments_dir, exist_ok=True)\n",
    "\n",
    "files = sorted(glob.glob(\"Target_proposals/*_with_discussion.csv\"))\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"Target_proposals/*_with_discussion.csv ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìƒì„± ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        needed = {\"space\",\"id\",\"title\",\"author\",\"discussion\",\"start\",\"end\",\"end_iso\",\"state\"}\n",
    "        if needed.issubset(df.columns):\n",
    "            dfs.append(df)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"ì½ì„ ìˆ˜ ìˆëŠ” with_discussion CSVê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "all_df = pd.concat(dfs, ignore_index=True)\n",
    "if len(all_df) == 0:\n",
    "    raise RuntimeError(\"with_discussion í–‰ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "sample_n = min(10, len(all_df))\n",
    "sample_df = all_df.sample(n=sample_n, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# -------- ì €ì¥ ì¤€ë¹„: í†µí•© ì•„ì›ƒí’‹ --------\n",
    "combined_csv_path = os.path.join(comments_dir, \"comments_all.csv\")\n",
    "combined_jsonl_path = os.path.join(comments_dir, \"comments_all.jsonl\")\n",
    "skip_log_path = os.path.join(comments_dir, \"skipped_non_discourse.csv\")\n",
    "\n",
    "# í†µí•© íŒŒì¼ ì´ˆê¸°í™”\n",
    "pd.DataFrame(columns=[\n",
    "    \"space\",\"proposal_id\",\"proposal_title\",\"discussion_url\",\n",
    "    \"post_id\",\"post_number\",\"reply_to_post_number\",\n",
    "    \"author_username\",\"created_at\",\"created_ts\",\"text_raw\",\"text_html\"\n",
    "]).to_csv(combined_csv_path, index=False)\n",
    "open(combined_jsonl_path, \"w\", encoding=\"utf-8\").close()\n",
    "pd.DataFrame(columns=[\n",
    "    \"space\",\"proposal_id\",\"proposal_title\",\"discussion_url\",\"reason\"\n",
    "]).to_csv(skip_log_path, index=False)\n",
    "\n",
    "print(f\"ğŸ“ ìƒ˜í”Œ {sample_n}ê°œ ëŒ“ê¸€ ì €ì¥ ì‹œì‘...\\n\")\n",
    "\n",
    "combined_rows = []\n",
    "skipped_rows = []\n",
    "\n",
    "for i in tqdm(range(sample_n), desc=\"Saving comments\"):\n",
    "    row = sample_df.loc[i]\n",
    "    url = str(row[\"discussion\"]).strip()\n",
    "    space = row[\"space\"]; pid = row[\"id\"]; title = row[\"title\"]\n",
    "    end_ts = int(row.get(\"end\", 0)) if pd.notna(row.get(\"end\", 0)) else 0\n",
    "\n",
    "    if not is_discourse_thread(url):\n",
    "        skipped_rows.append({\n",
    "            \"space\": space, \"proposal_id\": pid, \"proposal_title\": title,\n",
    "            \"discussion_url\": url, \"reason\": \"non-discourse (no structured comments)\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    data = fetch_discourse_thread(url, max_pages=10)\n",
    "    posts = data.get(\"posts\", [])\n",
    "    # í”„ë¡œí¬ì ˆ ì¢…ë£Œ ì´ì „ ëŒ“ê¸€ë§Œ\n",
    "    if end_ts:\n",
    "        posts = [p for p in posts if p.get(\"created_at\") and to_ts(p[\"created_at\"]) <= end_ts]\n",
    "\n",
    "    # ---- í”„ë¡œí¬ì ˆë³„ CSV ì €ì¥ ----\n",
    "    per_path = os.path.join(comments_dir, f\"{space}__{pid}__comments.csv\")\n",
    "    per_df = pd.DataFrame([{\n",
    "        \"space\": space,\n",
    "        \"proposal_id\": pid,\n",
    "        \"proposal_title\": title,\n",
    "        \"discussion_url\": url,\n",
    "        \"post_id\": p.get(\"id\"),\n",
    "        \"post_number\": p.get(\"post_number\"),\n",
    "        \"reply_to_post_number\": p.get(\"reply_to_post_number\"),\n",
    "        \"author_username\": p.get(\"username\"),\n",
    "        \"created_at\": p.get(\"created_at\"),\n",
    "        \"created_ts\": to_ts(p.get(\"created_at\") or \"\"),\n",
    "        \"text_raw\": p.get(\"raw\"),\n",
    "        \"text_html\": p.get(\"cooked\"),\n",
    "    } for p in posts])\n",
    "    per_df.to_csv(per_path, index=False)\n",
    "\n",
    "    # ---- í†µí•© CSV/JSONLì— ì¶”ê°€ ----\n",
    "    if len(posts):\n",
    "        combined_rows.extend(per_df.to_dict(orient=\"records\"))\n",
    "        with open(combined_jsonl_path, \"a\", encoding=\"utf-8\") as wf:\n",
    "            for p in posts:\n",
    "                record = {\n",
    "                    \"text\": p.get(\"raw\") or p.get(\"cooked\") or \"\",\n",
    "                    \"meta\": {\n",
    "                        \"space\": space,\n",
    "                        \"proposal_id\": pid,\n",
    "                        \"proposal_title\": title,\n",
    "                        \"discussion_url\": url,\n",
    "                        \"post_id\": p.get(\"id\"),\n",
    "                        \"post_number\": p.get(\"post_number\"),\n",
    "                        \"reply_to_post_number\": p.get(\"reply_to_post_number\"),\n",
    "                        \"author_username\": p.get(\"username\"),\n",
    "                        \"created_at\": p.get(\"created_at\"),\n",
    "                        \"created_ts\": to_ts(p.get(\"created_at\") or \"\"),\n",
    "                        \"text_raw\": p.get(\"raw\"),\n",
    "                        \"text_html\": p.get(\"cooked\"),\n",
    "                    }\n",
    "                }\n",
    "                wf.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# í†µí•© CSV ì“°ê¸°\n",
    "if combined_rows:\n",
    "    pd.DataFrame(combined_rows).to_csv(combined_csv_path, mode=\"w\", index=False)\n",
    "\n",
    "# ìŠ¤í‚µ ë¡œê·¸ ì“°ê¸°\n",
    "if skipped_rows:\n",
    "    pd.DataFrame(skipped_rows).to_csv(skip_log_path, mode=\"w\", index=False)\n",
    "\n",
    "print(\"\\nâœ… ì™„ë£Œ!\")\n",
    "print(f\" - í”„ë¡œí¬ì ˆë³„ ëŒ“ê¸€ CSV: Target_proposals/comments/<space>__<proposal_id>__comments.csv\")\n",
    "print(f\" - í†µí•© CSV: {combined_csv_path}\")\n",
    "print(f\" - í†µí•© JSONL(LLM ì¹œí™”): {combined_jsonl_path}\")\n",
    "print(f\" - ìŠ¤í‚µ ë¡œê·¸(ë¹„-ë””ìŠ¤ì½”ìŠ¤): {skip_log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
